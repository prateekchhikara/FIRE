{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Ingredients and Cooking Instructions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('sacrebleu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Ingredients Mapping to Check for Ingredient synonyms (cheddar cheese --> cheese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ingrs_mapping.json') as json_file:\n",
    "    original_dict = json.load(json_file)\n",
    "\n",
    "new_dict = {}\n",
    "\n",
    "for key, values in original_dict.items():\n",
    "    for value in values:\n",
    "        value = value.replace('_', ' ')\n",
    "        value = value.lower()\n",
    "        new_dict[value] = key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for Ingredients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ingredients(actual_file, pred_file, ret_metrics):\n",
    "    \"\"\"\"Summmary: Calculate the metrics for ingredients\n",
    "    Args:\n",
    "        actual_file: path to the file containing actual ingredients\n",
    "        pred_file: path to the file containing predicted ingredients\n",
    "        ret_metrics: dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    with open(actual_file, 'r') as f:\n",
    "        actual_ingredients = f.readlines()\n",
    "        #remove spaces and newlines\n",
    "        actual_ingredients = [x.strip() for x in actual_ingredients]\n",
    "\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predicted_ingredients = f.readlines()\n",
    "        #remove spaces and newlines\n",
    "        predicted_ingredients = [x.strip() for x in predicted_ingredients]\n",
    "    #if any of the actual_instruction and predicted_instruction are empty, return\n",
    "    if len (actual_ingredients) == 0 or len(predicted_ingredients) == 0:\n",
    "        return\n",
    "    \n",
    "    if predicted_ingredients[0] == '-1':\n",
    "        return\n",
    "    \n",
    "\n",
    "    #synonym replacement\n",
    "    for i in range(len(actual_ingredients)):\n",
    "        if actual_ingredients[i] in new_dict and new_dict[actual_ingredients[i]] in predicted_ingredients:\n",
    "            actual_ingredients[i] = new_dict[actual_ingredients[i]]\n",
    "\n",
    "    # print(f'predicted: {predicted_ingredients} \\n actual: {actual_ingredients}')\n",
    "\n",
    "    \n",
    "    #convert the list of ingredients to a set\n",
    "    actual_ingredients = set(actual_ingredients)\n",
    "    predicted_ingredients = set(predicted_ingredients)\n",
    "\n",
    "    # len(actual_ingredients.intersection(predicted_instruction)) / len(actual_ingredients)\n",
    "\n",
    "    try:\n",
    "        #find accuracy\n",
    "        accuracy = len(actual_ingredients.intersection(predicted_ingredients))/len(actual_ingredients)\n",
    "\n",
    "        #find precision\n",
    "        precision = len(actual_ingredients.intersection(predicted_ingredients))/len(predicted_ingredients)\n",
    "\n",
    "        #find recall\n",
    "        recall = len(actual_ingredients.intersection(predicted_ingredients))/len(actual_ingredients)\n",
    "\n",
    "        #find f1 score\n",
    "        f1 = 2*precision*recall/(precision+recall)\n",
    "\n",
    "        #find dice score\n",
    "        dice = 2*len(actual_ingredients.intersection(predicted_ingredients))/(len(actual_ingredients)+len(predicted_ingredients))\n",
    "\n",
    "        #find IoU\n",
    "        iou = len(actual_ingredients.intersection(predicted_ingredients))/len(actual_ingredients.union(predicted_ingredients))\n",
    "\n",
    "        ret_metrics['ingredient_accuracy'].append(accuracy)\n",
    "        ret_metrics['ingredient_precision'].append(precision)\n",
    "        ret_metrics['ingredient_recall'].append(recall)\n",
    "        ret_metrics['ingredient_f1'].append(f1)\n",
    "        ret_metrics['ingredient_dice'].append(dice)\n",
    "        ret_metrics['ingredient_iou'].append(iou)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'predicted: {predicted_ingredients} \\n actual: {actual_ingredients}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation for cooking instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recipe(actual_file, pred_file, ret_metrics):\n",
    "    \"\"\"\"Summmary: Calculate the metrics for recipe\n",
    "    Args:\n",
    "        actual_file: path to the file containing actual recipe\n",
    "        pred_file: path to the file containing predicted recipe\n",
    "        ret_metrics: dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(actual_file, 'r') as f:\n",
    "        actual_instruction = \"\"\n",
    "        actual_doc = f.readlines()\n",
    "        for line in actual_doc:\n",
    "            actual_instruction = actual_instruction + line.strip() + \" \"\n",
    "\n",
    "    with open(pred_file, 'r') as f:\n",
    "        predicted_instruction = \"\"\n",
    "        predicted_doc = f.readlines()\n",
    "        for line in predicted_doc:\n",
    "            predicted_instruction = predicted_instruction + line.strip() + \" \"\n",
    "        if predicted_instruction == '-1 ':\n",
    "            return\n",
    "        \n",
    "    #if any of the actual_instruction and predicted_instruction are empty, return\n",
    "    if not actual_instruction.strip() or not predicted_instruction.strip():\n",
    "        return\n",
    "            \n",
    "    try:\n",
    "        rouge_score = rouge.compute(predictions=[predicted_instruction], references=[[actual_instruction]])\n",
    "        blue_score = bleu.compute(predictions=[predicted_instruction], references = [[actual_instruction]])\n",
    "        # print(f'predicted: {predicted_instruction} \\n actual: {actual_instruction} \\n rouge: {rouge_score} \\n blue: {blue_score}')\n",
    "        # pdb.set_trace()\n",
    "        ret_metrics['recipe_rouge1'].append(rouge_score['rouge1'])\n",
    "        ret_metrics['recipe_rouge2'].append(rouge_score['rouge2'])\n",
    "        ret_metrics['recipe_rougeL'].append(rouge_score['rougeL'])\n",
    "        ret_metrics['recipe_bleu'].append(blue_score['score'])\n",
    "  \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(f'predicted: {predicted_instruction} \\n actual: {actual_instruction}')\n",
    "    # print(f'ret_metrics: {ret_metrics}')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the evaluation for ingredients and cooking instructions of each recipe in test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(GT, PRED):\n",
    "    \"\"\"Summary: Calculate the metrics for ingredients and recipe\n",
    "    Args:\n",
    "        GT: path to the file containing actual ingredients/recipe\n",
    "        PRED: path to the file containing predicted ingredients/recipe\n",
    "        typee: ingredients/recipe\n",
    "    \"\"\"\n",
    "    ret_metrics = {'recipe_bleu': [], 'recipe_rouge1': [], 'recipe_rouge2': [], 'recipe_rougeL': [], 'ingredient_accuracy': [], 'ingredient_precision': [], 'ingredient_recall': [], 'ingredient_f1': [], 'ingredient_dice': [], 'ingredient_iou': []}\n",
    "    \n",
    "    GT  = sorted(glob.glob(GT))\n",
    "    PRED  = sorted(glob.glob(PRED))\n",
    "    # count  = 0\n",
    "    for actual_file, pred_file in tqdm(zip(GT, PRED)):\n",
    "        evaluate_ingredients(actual_file, pred_file, ret_metrics)\n",
    "        evaluate_recipe(actual_file, pred_file, ret_metrics)\n",
    "\n",
    "    for k, v in ret_metrics.items():\n",
    "        if len(v) == 0:\n",
    "            ret_metrics[k] = 0\n",
    "        ret_metrics[k] = np.mean(v)\n",
    "    print(\"FINAL\", ret_metrics)\n",
    "    \n",
    "    return ret_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_PATH = \"GT/ingredients/*txt\"\n",
    "PRED_PATH = \"Predicted/ingredients/*\"\n",
    "\n",
    "print(evaluate_metrics(GT_PATH, PRED_PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invcook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
