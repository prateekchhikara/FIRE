{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recipe Title from Food Image (BLIP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "from difflib import SequenceMatcher\n",
    "import random  \n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"Summary: Custom Dataset for loading images and titles together\n",
    "    Args:\n",
    "        data_list (list): list of dictionaries containing image and title\n",
    "        processor (transformers.Processor): Processor for encoding the data\n",
    "    \"\"\"\n",
    "    def __init__(self, data_list, processor):\n",
    "        \"\"\"Summary: Constructor for CustomDataset\n",
    "        Args:\n",
    "            data_list (list): list of dictionaries containing image and title\n",
    "            processor (transformers.Processor): Processor for encoding the data\n",
    "        \"\"\"\n",
    "        self.data = data_list\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Summary: Returns the encoding of the image and title at the given index\n",
    "        Args:\n",
    "            index (int): index of the data\n",
    "            Returns:\n",
    "                encoding (dict): dictionary containing the encoding of the image and title\n",
    "        \"\"\"\n",
    "        item = self.data[index]\n",
    "        image = item['image']\n",
    "        title = item['title']\n",
    "        id = item[\"id\"]\n",
    "        \n",
    "        image_obj = Image.open(f\"train/{item['id']}/{item['image']}\")        \n",
    "        encoding = self.processor(images=image_obj, text=title, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        encoding = {k:v.squeeze() for k,v in encoding.items()}\n",
    "        \n",
    "        return encoding\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, processor, batch_size):\n",
    "    \"\"\"Summary: Loads the dataset from the given path\n",
    "    Args:\n",
    "        dataset_path (str): path to the dataset\n",
    "        processor (transformers.Processor): Processor for encoding the data\n",
    "        batch_size (int): batch size for the dataloader\n",
    "    Returns:\n",
    "        dataloader (torch.utils.data.DataLoader): dataloader for the dataset\n",
    "    \"\"\"\n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset_list = json.load(f)\n",
    "        \n",
    "    dataset = CustomDataset(dataset_list, processor)\n",
    "    shuffle = False\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    return dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Longest Common Subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    \"\"\"Summary: Calculate the longest common subsequence between two strings\n",
    "    Args:\n",
    "        a (str): First string\n",
    "        b (str): Second string\n",
    "    Returns:\n",
    "        float: Ratio of the longest common subsequence to the length of the longest string\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model based on Longest Common Subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, device, processor, dataset_path):\n",
    "    \"\"\"Summary: Evaluate the model on the given dataset\n",
    "    Args:\n",
    "        model (transformers.Model): Model to be evaluated\n",
    "        device (torch.device): Device to run the model on\n",
    "        processor (transformers.Processor): Processor for encoding the data\n",
    "        dataset_path (str): Path to the dataset\n",
    "    Returns:\n",
    "        float: Average similarity between the original and predicted titles\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(dataset_path, \"r\") as f:\n",
    "        dataset_list = json.load(f)\n",
    "        \n",
    "    \n",
    "    k = 1000\n",
    "    random.shuffle(dataset_list)\n",
    "\n",
    "    total_similarity = 0.0\n",
    "    total_samples = 0\n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        for item in tqdm(dataset_list[:k]):\n",
    "            image = item[\"image\"]\n",
    "            original_answers = item[\"title\"]\n",
    "            id = item['id']\n",
    "            \n",
    "            image_path = f\"test/{id}/{image}\" \n",
    "            image_obj = Image.open(image_path)\n",
    "            \n",
    "            inputs = processor(images=image_obj, return_tensors=\"pt\").to(device)\n",
    "            pixel_values = inputs.pixel_values\n",
    "            generated_ids = model.generate(pixel_values=pixel_values, max_length=10)\n",
    "            generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "            \n",
    "            # Calculate string similarity between original and predicted answers\n",
    "            batch_similarity = similar(original_answers, generated_caption)      \n",
    "            total_similarity += batch_similarity\n",
    "            total_samples += 1\n",
    "\n",
    "    average_similarity = total_similarity / total_samples\n",
    "\n",
    "    return average_similarity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer, processor, device):\n",
    "    \"\"\"Summary: Train the model on the given dataset and save the best models\n",
    "    Args:\n",
    "        model (transformers.Model): Model to be trained\n",
    "        train_dataloader (torch.utils.data.DataLoader): Dataloader for the training dataset\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for the model\n",
    "        processor (transformers.Processor): Processor for encoding the data\n",
    "        device (torch.device): Device to run the model on\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "    max_eval_score = 0  # Variable to track the best validation loss\n",
    "    best_model_state = None  # Variable to store the state of the best model\n",
    "    epochs = 10\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print(\"Epoch: \", epoch)\n",
    "\n",
    "        # Training loop\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            input_ids = batch.pop(\"input_ids\").to(device)\n",
    "            pixel_values = batch.pop(\"pixel_values\").to(device)\n",
    "\n",
    "            model.train()  # Set the model in training mode\n",
    "            outputs = model(input_ids=input_ids, pixel_values=pixel_values, labels=input_ids)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model in evaluation mode\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_score = evaluate(model, device, processor, \"dataset_test.json\")\n",
    "\n",
    "        print(\"Validation Score:\", eval_score)\n",
    "        \n",
    "        # Save the last model checkpoint\n",
    "        torch.save(model.state_dict(), \"last_model_checkpoint.pt\")\n",
    "\n",
    "        # Save the model if it has the best validation loss\n",
    "        if eval_score > max_eval_score:\n",
    "            max_eval_score = eval_score\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "            # Save the best model checkpoint\n",
    "            torch.save(best_model_state, \"best_model_checkpoint.pt\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best(device):\n",
    "    \"\"\"Summary: Load the best model checkpoint\n",
    "    Args:\n",
    "        device (torch.device): Device to run the model on\n",
    "    Returns:\n",
    "        model (transformers.Model): Model loaded from the best checkpoint\n",
    "    \"\"\"\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model.load_state_dict(torch.load(\"best_model_checkpoint.pt\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device(f\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 8\n",
    "    print(device) \n",
    "    processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\") \n",
    "    train_dataloader = load_dataset(\"dataset.json\", processor, batch_size)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    model.to(device)\n",
    "    train(model, train_dataloader, optimizer, processor, device)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invcook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
